#creating movies dataset
import requests
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

api_key = "ae60927b2621ffdf665f3b47791feda8"

def fetch_movie_details(movie_id):
    details_url = f"https://api.themoviedb.org/3/movie/{movie_id}?api_key={api_key}&append_to_response=credits"
    response = requests.get(details_url)
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Failed to fetch details for movie ID {movie_id}: {response.status_code}")
        return None

def fetch_data(api_key, page_limit=10):
    all_movies = []
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = []
        
        for page in range(1, page_limit + 1):
            print(f"Fetching data from page {page}")
            url = f"https://api.themoviedb.org/3/movie/popular?api_key={api_key}&language=en-US&page={page}"
            response = requests.get(url)
            
            if response.status_code != 200:
                print(f"Failed to fetch page {page}: {response.status_code}")
                continue
            
            results = response.json().get("results", [])
            
            for movie in results:
                movie_id = movie['id']
                futures.append(executor.submit(fetch_movie_details, movie_id))
        
        for future in as_completed(futures):
            details_response = future.result()
            if details_response:
                try:
                    title = details_response.get("title", "")
                    genres = ", ".join([g['name'] for g in details_response.get("genres", [])])
                    synopsis = details_response.get("overview", "")
                    rating = details_response.get("vote_average", 0)
                    credits = details_response.get("credits", {})
                    cast = credits.get("cast", [])
                    crew = credits.get("crew", [])
                    actors = ", ".join([person['name'] for person in cast[:3]])
                    directors = ", ".join([person['name'] for person in crew if person['job'] == "Director"])
                    
                    all_movies.append({
                        'title': title,
                        'genres': genres,
                        'actors': actors,
                        'directors': directors,
                        'synopsis': synopsis,
                        'rating': rating
                    })
                except Exception as e:
                    print(f"Error in processing data for movie: {e}")
                    continue
    
    # Check if any movies were collected
    if all_movies:
        return pd.DataFrame(all_movies)
    else:
        print("No movies were fetched.")
        return pd.DataFrame()  # Return an empty DataFrame if no movies were collected

# Fetch the data and create the dataset
movie_df = fetch_data(api_key, page_limit=10)

# Check if the DataFrame is not empty before saving
if not movie_df.empty:
    movie_df.to_csv('movies_dataset.csv', index=False)
    print("Movie Dataset is created")
else:
    print("No data to save. Dataset creation failed.")

#libraries
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
import pickle
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string



#Data cleaning
nltk.download('punkt')
nltk.download('stopwords')
data = pd.read_csv("movies_dataset.csv")
data.fillna('', inplace=True)
print("Columns in the DataFrame:", data.columns)


def preprocess_text(text):
    # Lowercasing
    text = text.lower()
    
    # Tokenization
    tokens = word_tokenize(text)
    
    # Removing stopwords and punctuation
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]
    
    return ' '.join(tokens)


column_name = 'description'  
if column_name in data.columns:
    data['processed_description'] = data[column_name].apply(preprocess_text)

    print(data[[column_name, 'processed_description']].head())
else:
    print(f"Column '{column_name}' does not exist in the DataFrame.")

data.head()

#Adding  extra columns
avg_actor_rating = data.groupby( 'actors' ) ['rating' ]. mean().to_dict()
avg_director_rating = data.groupby('directors' ) ['rating'].mean().to_dict()
data['avg_actor_rating'] = data['actors'].map(avg_actor_rating)
data['avg_director_rating'] = data['directors'].map(avg_director_rating)
data.head()

data = pd.read_csv("movies_dataset.csv")
data.fillna('', inplace=True)

print("Columns in the DataFrame:", data.columns)

text_column_name = 'synopsis' 
if text_column_name not in data.columns:
    print(f"Column '{text_column_name}' does not exist in the DataFrame.")
    exit()
bins = [0, 4, 6, 8, 10]  
labels = ['Low', 'Medium', 'High', 'Very High']  
data['rating_category'] = pd.cut(data['rating'], bins=bins, labels=labels, right=False)
y = data['rating_category']
X_train, X_test, y_train, y_test = train_test_split(data[text_column_name], y, test_size=0.2, random_state=42)

#vectorizer
vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

#SentenceTransformer
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
synopsis_embeddings = sentence_model.encode(data['synopsis'].tolist())
genre_ohe = pd.get_dummies (data['genres'])
x = np.hstack((
synopsis_embeddings,
genre_ohe.values,
data[['avg_actor_rating', 'avg_director_rating']].values))
y = data['rating']
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)

# XGBRegressor
xgb_model = XGBRegressor(
n_estimators = 300,
learning_rate = 0.03,
max_depth = 7,
subsample=0.8,
colsample_bytree=0.8,
random_state=42
)
xgb_model.fit(x_train,y_train)
y_pred = xgb_model.predict(x_test)


# Logistic Regression
logistic_model = LogisticRegression()
logistic_model.fit(X_train_tfidf, y_train)
y_pred_logistic = logistic_model.predict(X_test_tfidf)

print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_logistic))
print(classification_report(y_test, y_pred_logistic))

# Naïve Bayes
naive_bayes_model = MultinomialNB()
naive_bayes_model.fit(X_train_tfidf, y_train)
y_pred_naive_bayes = naive_bayes_model.predict(X_test_tfidf)

print("Naïve Bayes Accuracy:", accuracy_score(y_test, y_pred_naive_bayes))
print(classification_report(y_test, y_pred_naive_bayes))


#SVC
svm_model = SVC()
svm_model.fit(X_train_tfidf, y_train)
y_pred_svm = svm_model.predict(X_test_tfidf)

print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))


#R2 SCore
r2 = r2_score(y_test,y_pred)
mse = mean_squared_error(y_test,y_pred)
print(f"r2 Score {r2:.4f}")
print(f"Mean Squared Error: {mse: .4f}")

#saving models
pickle.dump(xgb_model, open('movie_rating_model.pkl', 'wb'))
pickle.dump(sentence_model, open('Sentence_model.pkl', 'wb'))
pickle.dump(genre_ohe.columns.to_list(), open("genre_columns.pkl", 'wb'))
pickle.dump(avg_actor_rating, open("avg_actor_rating.pkl", 'wb'))
pickle.dump(avg_director_rating, open("avg_director_rating.pkl", 'wb'))


